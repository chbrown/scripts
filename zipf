#!/usr/bin/env python
import re
import sys
import argparse
from collections import Counter


def tokenize(string):
    '''
    Find all consecutive sequences of words characters in `string`,
    yielding strings.
    '''
    for match in re.finditer(r'\w+', string, re.UNICODE):
        yield match.group(0)


def print_table(counter, n):
    '''
    Print out whitespace-aligned table of `n` most common tokens,
    along with their raw count and overall frequency.
    '''
    total = sum(counter.values())
    print('{:26s} {:7s} {:s}'.format('token', 'count', 'freq'))
    for token, count in counter.most_common(n):
        print('{:26s} {:7d} {:8.4%}'.format(token, count, count / total))


def main():
    parser = argparse.ArgumentParser(
        description='Token / type comparator',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-t', '--top', default=200, type=int, help='output lines')
    opts = parser.parse_args()

    counter = Counter()
    # line breaks don't matter, but they are a convenient way to iterate
    for line in sys.stdin:
        tokens = tokenize(line)
        counter.update(tokens)

    # print report to default output
    print_table(counter, opts.top)


if __name__ == '__main__':
    main()
